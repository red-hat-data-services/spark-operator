# Kubeflow Spark Operator on Red Hat AI

This documentation details how the Kubeflow Spark Operator works on Red Hat AI, its architecture, installation, and how to run a distributed Spark workload using the `docling-spark` application with PVC-based storage.

## 1. Spark Operator Architecture

The Spark Operator follows the standard Kubernetes Operator pattern:

1.  **CRD Controller**: Watches for events (Create, Update, Delete) on `SparkApplication` resources across configured namespaces.
2.  **Submission Runner**: When a `SparkApplication` is created, the operator generates the `spark-submit` command and executes it inside a simplified "submission" pod or internally.
3.  **Spark Pod Monitor**: Watches the status of the Driver and Executor pods and updates the `.status` field of the `SparkApplication` resource.
4.  **Mutating Admission Webhook**: An optional but recommended component that intercepts pod creation requests. It injects Spark-specific configuration (like mounting ConfigMaps or Volumes) into the Driver and Executor pods before they are scheduled.

### Flow on OpenShift

> **Note:** The Operator must be installed by a Cluster Admin before users can submit jobs.

1.  User applies `SparkApplication` YAML.
2.  Operator Controller detects the new resource.
3.  Operator creates a **Driver Pod** in the target namespace via the OpenShift Cluster.
4.  Driver Pod starts and requests **Executor Pods** from the OpenShift Cluster.
5.  Executor Pods start, connect to the Driver, and process the tasks.

![Spark Operator Flow on OpenShift](diagrams/Explanation_diagram.png)

## 2. Installation on OpenShift

> **Pre-requisite:** This section requires **Cluster Admin** privileges. You must install the operator once so that users can submit `SparkApplication` CRDs.

### Prerequisites
*   OpenShift CLI (`oc`) configured
*   Cluster Admin privileges
*   Git (to clone the repository)

> **Note:** Docker and a container registry (e.g., Quay.io) are only needed if you plan to build custom Spark images. See [Building Custom Spark Images for OpenShift](./BuildingCustomSparkImages.md).

### Installation Steps

The operator is installed using Kustomize manifests located in `config/`. This approach is GitOps-friendly, declarative, and follows the OpenDataHub operator pattern.

#### 1. Clone the Repository

```bash
git clone https://github.com/opendatahub-io/spark-operator.git
cd spark-operator
```

#### 2. Login to OpenShift

```bash
# Log in to your Red Hat OpenShift cluster
oc login 
```

#### 3. Architecture

```
config/
â”œâ”€â”€ component_metadata.yaml     # Version tracking
â”œâ”€â”€ crd/                        # Custom Resource Definitions
â”‚   â”œâ”€â”€ bases/                  # CRD YAML files (single source of truth)
â”‚   â”œâ”€â”€ kustomization.yaml
â”‚   â””â”€â”€ kustomizeconfig.yaml
â”œâ”€â”€ manager/                    # Controller manager deployment
â”‚   â”œâ”€â”€ kustomization.yaml
â”‚   â”œâ”€â”€ manager.yaml            # Namespace + Deployment combined
â”‚   â””â”€â”€ serviceaccount.yaml
â”œâ”€â”€ webhook/                    # Admission webhook
â”‚   â”œâ”€â”€ kustomization.yaml
â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”œâ”€â”€ service.yaml
â”‚   â”œâ”€â”€ serviceaccount.yaml
â”‚   â”œâ”€â”€ clusterrole.yaml
â”‚   â”œâ”€â”€ clusterrolebinding.yaml
â”‚   â”œâ”€â”€ role.yaml
â”‚   â”œâ”€â”€ rolebinding.yaml
â”‚   â”œâ”€â”€ mutatingwebhookconfiguration.yaml   # Custom webhook config (used by kustomize)
â”‚   â”œâ”€â”€ validatingwebhookconfiguration.yaml # Custom webhook config (used by kustomize)
â”‚   â””â”€â”€ manifests.yaml          # Auto-generated by controller-gen (CI only, not used)
â”œâ”€â”€ rbac/                       # All RBAC consolidated
â”‚   â”œâ”€â”€ kustomization.yaml
â”‚   â”œâ”€â”€ clusterrole.yaml        # Custom ClusterRole (used by kustomize)
â”‚   â”œâ”€â”€ clusterrolebinding.yaml
â”‚   â”œâ”€â”€ leader-election-role.yaml  # Namespace-scoped Role for leader election
â”‚   â”œâ”€â”€ role.yaml               # Auto-generated by controller-gen (CI only, not used)
â”‚   â”œâ”€â”€ rolebinding.yaml
â”‚   â”œâ”€â”€ spark_serviceaccount.yaml
â”‚   â”œâ”€â”€ spark_role.yaml
â”‚   â”œâ”€â”€ spark_rolebinding.yaml
â”‚   â””â”€â”€ *_viewer_role.yaml / *_editor_role.yaml
â”œâ”€â”€ default/                    # Main entry point (development/standalone)
â”‚   â””â”€â”€ kustomization.yaml
â””â”€â”€ overlays/                   # Environment-specific configurations
    â”œâ”€â”€ odh/                    # OpenDataHub (namespace: opendatahub)
    â”‚   â”œâ”€â”€ kustomization.yaml
    â”‚   â”œâ”€â”€ params.env
    â”‚   â””â”€â”€ delete-namespace.yaml
    â””â”€â”€ rhoai/                  # Red Hat OpenShift AI (namespace: redhat-ods-applications)
        â”œâ”€â”€ kustomization.yaml
        â”œâ”€â”€ params.env
        â””â”€â”€ delete-namespace.yaml
```

#### 4. Install the Operator

```bash
oc apply -k config/default/ --server-side=true
```

> **Note:** The `--server-side=true` flag is required because the CRDs are large and exceed Kubernetes annotation size limits for client-side apply.

This creates:
- Operator namespace with controller and webhook deployments
- 3 CRDs (SparkApplication, ScheduledSparkApplication, SparkConnect)
- Comprehensive RBAC configuration
- Spark job ServiceAccount for driver pods

> **Version Note:** The Kustomize manifests use Spark Operator v2.4.0 which supports Spark 3.5.x. Our docling-spark application uses Spark 3.5.7 with Java 17 for Python 3.10 compatibility (required by docling). Newer operator versions (v2.5.x+) may ship with Spark 4.x which could have breaking changes. See the [version matrix](https://github.com/kubeflow/spark-operator?tab=readme-ov-file#version-matrix) for details.
>
> **Multiple Namespaces:** By default, the operator watches all namespaces (empty `--namespaces=` flag). To watch specific namespaces, modify the `--namespaces` argument in `config/manager/manager.yaml` to `--namespaces=ns1,ns2,ns3`.

#### 5. Verify Installation

```bash
oc get pods -n spark-operator -l app.kubernetes.io/name=spark-operator

# Expected output:
# NAME                                        READY   STATUS    RESTARTS   AGE
# spark-operator-controller-xxx               1/1     Running   0          1m
# spark-operator-webhook-xxx                  1/1     Running   0          1m
```

#### 6. Verify Security Context (OpenShift)

Confirm that the `restricted-v2` SCC is assigned:

```bash
oc describe pod -n spark-operator -l app.kubernetes.io/component=controller | grep -i openshift.io/scc
# Expected: openshift.io/scc: restricted-v2
```

Verify the container runs with a non-root UID:

```bash
POD=$(oc get pod -n spark-operator -l app.kubernetes.io/component=controller -o jsonpath='{.items[0].metadata.name}')
oc exec -n spark-operator $POD -- id
# Expected: uid=1000xxx gid=0(root) groups=0(root),1000xxx
```

### Uninstall

```bash
oc delete -k config/default/
```

### (Optional) Custom ServiceAccount for Spark Jobs

The operator installation creates a default ServiceAccount called `spark-operator-spark` in the operator namespace. This is sufficient for most use cases.

However, the `examples/openshift/k8s/base/` directory provides an **alternative custom ServiceAccount** (`spark-driver`) that you can use when you need:

- **Custom naming**: Use `spark-driver` instead of `spark-operator-spark` for organizational clarity
- **Additional permissions**: The custom SA includes `pods/log` (debugging) and `secrets` (read-only credentials) access

#### Using the Default ServiceAccount

No additional setup needed. In your SparkApplication:

```yaml
spec:
  driver:
    serviceAccount: spark-operator-spark
```

#### Using the Custom ServiceAccount

1. Apply the custom RBAC:

First, create the required namespaces:

```bash
oc apply -k examples/openshift/k8s/base/
```

This creates:
- `spark-driver` ServiceAccount
- `spark-role` Role with extended permissions
- `spark-role-binding` RoleBinding
- `ValidatingAdmissionPolicy` to reject `fsGroup` in SparkApplications (security feature)

2. Reference it in your SparkApplication:

```yaml
spec:
  driver:
    serviceAccount: spark-driver
```

#### What's in `k8s/base/`?

```text
k8s/base/
â”œâ”€â”€ kustomization.yaml                      # Kustomize entry point
â”œâ”€â”€ namespace.yaml                          # Creates spark-operator namespace (usually not needed)
â”œâ”€â”€ rbac.yaml                               # Custom spark-driver SA with extended permissions
â”œâ”€â”€ validating-admission-policy.yaml        # Rejects fsGroup in SparkApplications
â””â”€â”€ validating-admission-policy-binding.yaml
```

> **Note:** The `k8s/base/` files are **optional**. The operator installation (`config/default/`) already creates the `spark-operator-spark` ServiceAccount which is sufficient for most use cases. The custom `spark-driver` ServiceAccount is only needed if you require additional permissions (pods/log, secrets access).

2. Confirm Security Context Constraint (SCC)
Verify that the `restricted-v2` policy is assigned to the pods:

```bash
oc describe pod <POD_NAME> -n spark-operator | grep -i openshift.io/scc
```

Expected output:
```
openshift.io/scc: restricted-v2
```

3. Verify Arbitrary UID Injection (Acceptance Criteria)
To definitively prove that the container is running with a random non-root UID and is a member of the required Group 0, execute the id command inside the container. This confirms the environment is ready for the compatible Spark image.
```bash
oc exec -n spark-operator <POD_NAME> -- id
```

## 3. SparkApplication CRD

The **SparkApplication** Custom Resource Definition (CRD) is the core abstraction provided by the operator. It allows you to define Spark applications declaratively using Kubernetes YAML manifests, similar to how you define Deployments or Pods.

Key fields in the `SparkApplication` spec include:

*   **`type`**: The language of the application (`Python`).
*   **`mode`**: Deployment mode (`cluster` or `client`). In `cluster` mode, the driver runs in a pod.
*   **`image`**: The container image to use for the driver and executors.
*   **`mainApplicationFile`**: The entry point path (e.g., `local:///app/scripts/run_spark_job.py`).
*   **`sparkVersion`**: The version of Spark to use (must match the image).
*   **`restartPolicy`**: Handling of failures (`Never`, `OnFailure`, `Always`).
*   **`driver` / `executor`**: Resource requests (cores, memory), labels, service accounts, and **security contexts**.
*   **`volumes` / `volumeMounts`**: PVCs for input and output data.

> **Security Note:** The SparkApplication does NOT set `fsGroup`, `runAsUser`, or `runAsGroup`. OpenShift's `restricted-v2` SCC assigns these automatically, and the CSI driver honors them.

Example snippet from `k8s/docling-spark-app.yaml`:

> **Important:** The SparkApplication runs in the **same namespace** as the operator. Update the `namespace` field based on your deployment overlay.

```yaml
apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: docling-spark-job
  namespace: spark-operator  # Same as operator namespace
spec:
  type: Python
  pythonVersion: "3"
  mode: cluster
  image: quay.io/rishasin/docling-spark:multi-output
  imagePullPolicy: Always
  mainApplicationFile: local:///app/scripts/run_spark_job.py
  arguments:
    - "--input-dir"
    - "/app/assets"
    - "--output-file"
    - "/app/output/results.jsonl"
  sparkVersion: "3.5.7"
  restartPolicy:
    type: Never
  driver:
    cores: 1
    memory: "4g"
    serviceAccount: spark-operator-spark  # Or spark-driver if using custom RBAC
    securityContext: {}
  executor:
    cores: 1
    instances: 2
    memory: "4g"
    securityContext: {}
```

> **Note:** See `k8s/docling-spark-app.yaml` for the complete configuration including `timeToLiveSeconds` and labels.
> **Note:** To ensure compatibility with OpenShift's default restricted-v2 Security Context Constraint (SCC), the explicit securityContext block (including runAsNonRoot, fsGroup, etc.) has been removed from both the driver and executor specifications in k8s/docling-spark-app.yaml.

### Admission Control Policy

To prevent users from setting `fsGroup` in SparkApplication specs, install a ValidatingAdmissionPolicy:

```bash
# As cluster admin
oc apply -f examples/openshift/k8s/base/validating-admission-policy.yaml
oc apply -f examples/openshift/k8s/base/validating-admission-policy-binding.yaml
```

> Note: To Disable the policy: `oc delete validatingadmissionpolicybinding deny-fsgroup-in-sparkapplication-binding`

## 4. About Docling-Spark Application

The `docling-spark` application demonstrates a production-grade pattern for processing documents at scale using:
*   **Docling**: For advanced document layout analysis and understanding.
*   **Apache Spark**: For distributed processing across the cluster.
*   **Kubeflow Spark Operator**: For native Kubernetes lifecycle management.
*   **PVC-based Storage**: Input and output data stored on persistent volumes.

### How It Works
1.  Upload PDFs to the **input PVC**.
2.  **Spark Operator** launches a Driver Pod.
3.  **Driver** reads files from input PVC, distributes work to Executor Pods.
4.  **Executors** process PDFs in parallel (OCR, Layout Analysis, Table Extraction).
5.  **Driver** collects results and writes to **output PVC**.
6.  Download results from output PVC anytime.

## 5. Deploying the Docling-Spark Application

This section uses the **pre-built image** `quay.io/rishasin/docling-spark:latest` which contains the Docling + PySpark application with all dependencies. The manifest `k8s/docling-spark-app.yaml` is already configured to use this image.

> **Important:** The Spark job runs in the **same namespace** as the operator. The deploy script uses the `spark-operator-spark` ServiceAccount that is created during operator installation.

> **Building Custom Images?** If you need custom dependencies or want to use your own container registry, see [Building Custom Spark Images for OpenShift](./BuildingCustomSparkImages.md) for best practices on OpenShift compatibility and build instructions.

### Step 1: Upload Your PDFs

```bash
# Make the script executable (first time only)
chmod +x k8s/deploy.sh

# Upload your PDF files to the input PVC
./k8s/deploy.sh upload ./path/to/your/pdfs/
```


### Step 2: Run the Spark Job
The deploy script creates PVCs and submits the SparkApplication in the operator namespace:

```bash
./k8s/deploy.sh
```

Expected output:

```text
==============================================
  Deploying Docling + PySpark
==============================================
  Namespace: spark-operator
==============================================

[INFO] 1. Verifying namespace exists...
[OK] Namespace 'spark-operator' exists

[INFO] 2. Ensuring PVCs exist...
persistentvolumeclaim/docling-input created
persistentvolumeclaim/docling-output created

3. Installing ValidatingAdmissionPolicy (optional)...
   âš ï¸  Skipping (requires cluster-admin). Install manually if needed.

4. Submitting Spark Application...
sparkapplication.sparkoperator.k8s.io/docling-spark-job created

[OK] Deployment complete!

ðŸ“Š Check status:
   oc get sparkapplications -n spark-operator
   oc get pods -n spark-operator -w

ðŸ“ View logs:
   oc logs -f docling-spark-job-driver -n spark-operator

ðŸŒ Access Spark UI (when driver is running):
   oc port-forward -n spark-operator svc/docling-spark-job-ui-svc 4040:4040
   Open: http://localhost:4040
```

> **Note:** On subsequent runs, you'll see `unchanged` instead of `created` for resources that already exist.

### Step 3: Monitor the Job

```bash
# Watch pods (adjust namespace based on your deployment)
oc get pods -n spark-operator -w
```

Expected output (pods lifecycle):

```text
NAME                                        READY   STATUS              AGE
docling-spark-job-driver                    0/1     Pending             0s
docling-spark-job-driver                    0/1     ContainerCreating   0s
docling-spark-job-driver                    1/1     Running             2s
doclingsparkjob-xxx-exec-1                  0/1     Pending             0s
doclingsparkjob-xxx-exec-1                  0/1     ContainerCreating   0s
doclingsparkjob-xxx-exec-1                  1/1     Running             3s
doclingsparkjob-xxx-exec-2                  1/1     Running             3s
...
doclingsparkjob-xxx-exec-1                  0/1     Completed           83s
doclingsparkjob-xxx-exec-2                  0/1     Completed           83s
docling-spark-job-driver                    0/1     Completed           100s
```

View application logs:
```bash
# Adjust namespace based on your deployment
oc logs -f docling-spark-job-driver -n spark-operator
```

Expected output:

```text
======================================================================
ðŸ“„ ENHANCED PDF PROCESSING WITH PYSPARK + DOCLING
======================================================================
Creating Spark session...
Spark session created with 2 workers

ðŸ“‚ Step 1: Getting list of PDF files...
   Looking for PDFs in: /app/assets
âœ… Found PDF: document1.pdf
âœ… Found PDF: document2.pdf
   Found 2 files to process

ðŸ”„ Step 3: Processing files (this is where the magic happens!)...
   Spark is now distributing work to workers...

ðŸ“Š Step 4: Organizing results...

âœ… Step 5: Results are ready! (Count: 2)

ðŸ’¾ Step 6: Saving results to JSONL file...
âœ… Results saved to: /app/output/results.jsonl

ðŸŽ‰ ALL DONE!
âœ… Enhanced processing complete!
ðŸ“¦ Results are stored on the output PVC.
   To download: ./k8s/deploy.sh download ./output/
```

### Step 4: Download the Results

First, delete the SparkApplication to release the output PVC:
```bash
# Adjust namespace based on your deployment
oc delete sparkapplication docling-spark-job -n spark-operator
```

Expected output:

```text
sparkapplication.sparkoperator.k8s.io "docling-spark-job" deleted
```

Download results from the output PVC:
```bash
./k8s/deploy.sh download ./output/
```

Expected output:

```text
==============================================
  Downloading results from Output PVC
==============================================

[INFO] Creating helper pod 'pvc-downloader'...
pod/pvc-downloader created
[INFO] Waiting for pod to be ready...
pod/pvc-downloader condition met
[OK] Helper pod ready
[INFO] Files on output PVC:
-rw-rw-r--. 1 1000840000 1000840000 2140488 Dec 15 03:55 results.jsonl
[INFO] Copying files to './output/'...

[INFO] Downloaded files:
-rw-r--r--  1 user  staff  2140488 Dec 15 03:55 results.jsonl
[OK] Download complete!
[INFO] Deleting helper pod 'pvc-downloader'...
pod "pvc-downloader" deleted
```

View results:
```bash
cat ./output/results.jsonl
```


### Step 5: Access Spark UI (Optional)

While the driver is running:
```bash
# Adjust namespace based on your deployment
oc port-forward -n spark-operator svc/docling-spark-job-ui-svc 4040:4040
# Open: http://localhost:4040
```

## 6. Quick Reference

| Action | Command |
|--------|---------|
| Install operator | `oc apply -k config/default/ --server-side=true` |
| Uninstall operator | `oc delete -k config/default/` |
| Upload PDFs | `./k8s/deploy.sh upload ./my-pdfs/` |
| Run job | `./k8s/deploy.sh` (creates PVCs and submits job) |
| Check status | `./k8s/deploy.sh status` |
| View logs | `oc logs -f docling-spark-job-driver -n spark-operator` |
| Delete job | `oc delete sparkapplication docling-spark-job -n spark-operator` |
| Download results | `./k8s/deploy.sh download ./output/` |
| Cleanup helpers | `./k8s/deploy.sh cleanup` |

## 7. Debugging and Logging

### Operator Logs
If your Spark jobs are not starting (e.g., no pods created), check the operator logs:

```bash
oc logs -n spark-operator -l app.kubernetes.io/name=spark-operator
```

### Application Logs
Once the Driver pod is created, check its logs for Spark-specific initialization and application output:

```bash
oc logs docling-spark-job-driver -n spark-operator
oc logs docling-spark-job-exec-1 -n spark-operator
```

### SparkApplication Status
Inspect the status of the CRD to see if the operator encountered validation errors or submission failures:

```bash
# Adjust namespace based on your deployment
oc describe sparkapplication docling-spark-job -n spark-operator
```

### CSI Driver Check
Verify the CSI driver supports fsGroup:
```bash
oc get csidriver ebs.csi.aws.com -o yaml | grep fsGroupPolicy
# Expected: fsGroupPolicy: File
```

## 8. Docling-Spark Application Files

| Path | Description |
|------|-------------|
| `examples/openshift/k8s/base/` | **Optional**: Custom `spark-driver` SA + ValidatingAdmissionPolicy |
| `examples/openshift/k8s/docling-spark-app.yaml` | SparkApplication manifest |
| `examples/openshift/k8s/docling-input-pvc.yaml` | PVC for input data (10Gi default) |
| `examples/openshift/k8s/docling-output-pvc.yaml` | PVC for output data (10Gi default) |
| `examples/openshift/k8s/deploy.sh` | Deployment script (deploy, upload, download) |
| `examples/openshift/scripts/run_spark_job.py` | PySpark driver script |
| `examples/openshift/scripts/docling_module/` | PDF processing logic |
| `examples/openshift/Dockerfile` | Container image definition |
| `examples/openshift/requirements-docker.txt` | Docker container dependencies |

> **Storage Note:** Both PVCs default to 10Gi. For text + metadata outputs, this is typically sufficient since output is often smaller than the source PDFs. However, if you plan to export additional artifacts (images, per-page outputs, multiple formats like JSON/MD/HTML), increase the output PVC size accordingly.

## 9. Cleanup

```bash
# Delete the SparkApplication
oc delete sparkapplication docling-spark-job -n spark-operator

# Delete PVCs (WARNING: This deletes all data!)
oc delete pvc docling-input docling-output -n spark-operator

# Delete the operator
oc delete -k config/default/
```

## 10. Updating Manifests from Upstream

When syncing this repository with the upstream Kubeflow Spark Operator, the Kustomize manifests need to be updated. See [UPDATING_MANIFESTS.md](./UPDATING_MANIFESTS.md) for detailed instructions.

### References
*   [Red Hat Developer: Raw Data to Model Serving](https://developers.redhat.com/articles/2025/07/29/raw-data-model-serving-openshift-ai)
*   [Red Hat Access: Spark Operator on OpenShift](https://access.redhat.com/articles/7131048)
*   [Kubeflow Spark Operator GitHub](https://github.com/kubeflow/spark-operator)
